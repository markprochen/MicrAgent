import os
import re
import json
import ast
from typing import Annotated, List, Union, TypedDict
from datetime import datetime
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver  # å¯¼å…¥æŒä¹…åŒ–è®°å¿†

from mcp_manage import MCPManager
from ModelManager import ModelManager

load_dotenv()

# --- 1. åˆå§‹åŒ–ç®¡ç†ä¸­å¿ƒ ---
mcp_manager = MCPManager()
# åˆå§‹å·¥å…·é›†
ALL_TOOLS = {
    "get_skill_detail": mcp_manager.get_skill_detail,
    **mcp_manager.tools
}

# --- 2. å®šä¹‰çŠ¶æ€ ---
class AgentState(TypedDict):
    # add_messages ä¼šå°†æ–°æ¶ˆæ¯è¿½åŠ åˆ°å†å²ä¸­
    messages: Annotated[List[BaseMessage], add_messages]
    # next_model å°†ç”± execute_tool_node æ›´æ–°å¹¶æŒä¹…åŒ–
    next_model: str 

# --- 3. æ¨¡å‹ç®¡ç† ---
model_manager = ModelManager()

# --- 4. èŠ‚ç‚¹å®šä¹‰ ---
def call_agent_node(state: AgentState):
    # 1. è·å–å½“å‰ç”¨æˆ·é€‰å®šçš„æ¨¡å‹
    target_model_id = state.get("next_model") or "reasoner"
    llm = model_manager.get_model(target_model_id)
    
    # 2. æ¸…æ´—å†å²ä¸Šä¸‹æ–‡ (Context Cleaning)
    # å¦‚æœå½“å‰ä¸æ˜¯ R1 æ¨¡å‹ï¼Œæˆ‘ä»¬æŠŠå†å²æ¶ˆæ¯é‡Œçš„ <think> æ ‡ç­¾å…¨éƒ¨åˆ æ‰å†å‘ç»™å®ƒ
    processed_messages = []
    for msg in state["messages"]:
        if isinstance(msg, (HumanMessage, SystemMessage)):
            processed_messages.append(msg)
        elif hasattr(msg, "content"):
            # å¦‚æœæ˜¯ AI çš„æ¶ˆæ¯ï¼Œä¸”å½“å‰æ¨¡å‹ä¸æ˜¯ reasoner (å‡è®¾åªæœ‰ reasoner ä¼šäº§å‡º think)
            if target_model_id != "reasoner":
                clean_content = re.sub(r"<think>.*?</think>", "", msg.content, flags=re.DOTALL).strip()
                processed_messages.append(msg.__class__(content=clean_content))
            else:
                processed_messages.append(msg)

    # 3. åŠ è½½ manifests... (ä¿æŒä¸å˜)
    base_info = mcp_manager.load_static_md("base.md")
    manifest = mcp_manager.load_static_md("manifest.md")
    
    system_prompt = f"{base_info}\n\n[å½“å‰å¤§è„‘]: {target_model_id}\n\n{manifest}"
    
    # ä½¿ç”¨æ¸…æ´—åçš„æ¶ˆæ¯å‘é€ç»™ LLM
    final_input = [SystemMessage(content=system_prompt)] + processed_messages
    
    response = llm.invoke(final_input)
    return {"messages": [response]}
def execute_tool_node(state: AgentState):
    last_message = state["messages"][-1]
    content = last_message.content
    
    # ä¾ç„¶ä¿ç•™ä» state è·å– next_model çš„é€»è¾‘ï¼Œä¿è¯æ¨¡å‹çŠ¶æ€åœ¨å›¾ä¸­æµè½¬
    current_model = state.get("next_model", "reasoner")

    # è§£æ Action / Input (ä¿æŒåŸæ¥çš„æ­£åˆ™å’Œ Payload é€»è¾‘)
    # ... (çœç•¥è§£æä»£ç ) ...

    # æ‰§è¡Œå®Œæˆå
    return {
        "messages": [HumanMessage(content=f"å·¥å…·æ‰§è¡Œåé¦ˆ...")],
        "next_model": current_model # ä¿æŒç”¨æˆ·é€‰æ‹©çš„æ¨¡å‹ï¼Œä¸è¦å»ä¿®æ”¹å®ƒ
    }

# --- 5. æ„å»ºå›¾ ---
workflow = StateGraph(AgentState)

workflow.add_node("agent", call_agent_node)
workflow.add_node("tools", execute_tool_node)

workflow.add_edge(START, "agent")
# æ¡ä»¶è¾¹ï¼šæ ¹æ®æ¨¡å‹è¾“å‡ºæ˜¯å¦åŒ…å« Action å†³å®šå»å·¥å…·èŠ‚ç‚¹è¿˜æ˜¯ç»“æŸ
workflow.add_conditional_edges(
    "agent", 
    lambda x: "tools" if "Action:" in x["messages"][-1].content else END
)
workflow.add_edge("tools", "agent")

# å¯ç”¨æŒä¹…åŒ–è®°å¿†
checkpointer = MemorySaver()
app = workflow.compile(checkpointer=checkpointer)

# --- 6. äº¤äº’å…¥å£ ---
if __name__ == "__main__":
    # 1. å¯åŠ¨æ—¶æ˜¾ç¤ºå¯ç”¨æ¨¡å‹åˆ—è¡¨ (ç”± ModelManager åŠ¨æ€ç”Ÿæˆ)
    print(model_manager.get_models_menu())
    
    # 2. åˆå§‹åŒ–é»˜è®¤æ¨¡å‹
    # å¯»æ‰¾é…ç½®æ–‡ä»¶ä¸­æ ‡è®°ä¸º default çš„ ID
    current_model_id = "reasoner"
    for cfg in model_manager.config['models']:
        if cfg.get('default'):
            current_model_id = cfg['id']
            break
    
    # ä¸º LangGraph å‡†å¤‡æŒä¹…åŒ–é…ç½®
    config = {"configurable": {"thread_id": "Wink_User_Session"}}
    
    print(f"âœ… ç³»ç»Ÿå°±ç»ªã€‚å½“å‰é»˜è®¤å¤§è„‘: [{current_model_id}]")

    while True:
        prompt_str = f"\n({current_model_id}) ç”¨æˆ· > "
        query = input(prompt_str).strip()
        
        if not query:
            continue
        if query.lower() in ["exit", "quit", "q"]:
            break

        # --- 3. åŠ¨æ€è·¯ç”±æŒ‡ä»¤å¤„ç† ---
        if query.startswith("/"):
            parts = query.split()
            cmd = parts[0].lower()
            
            # /list æŒ‡ä»¤ï¼šé‡æ–°æ˜¾ç¤ºèœå•
            if cmd == "/list":
                print(model_manager.get_models_menu())
                continue
            
            # /use [id] æŒ‡ä»¤ï¼šåˆ‡æ¢æ¨¡å‹
            elif cmd == "/use" and len(parts) > 1:
                target_id = parts[1].lower()
                if target_id in model_manager.get_all_ids():
                    current_model_id = target_id
                    print(f"ğŸ§  å·²åˆ‡æ¢å¤§è„‘ä¸º: [{current_model_id}]")
                else:
                    print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° ID ä¸º '{target_id}' çš„æ¨¡å‹ã€‚è¾“å…¥ /list æŸ¥çœ‹å¯ç”¨ IDã€‚")
                continue
            
            else:
                print("â“ æœªçŸ¥æŒ‡ä»¤ã€‚å¯ç”¨æŒ‡ä»¤: /list, /use [id]")
                continue

        # --- 4. è¿è¡Œ Agent å›¾ ---
        # æ¯æ¬¡è¿è¡Œå‰ï¼Œå°†å½“å‰é€‰å®šçš„ current_model_id æ”¾å…¥ initial_state
        initial_state = {
            "messages": [HumanMessage(content=query)],
            "next_model": current_model_id 
        }

        # è¿è¡Œæµ
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ stream_mode="values" æˆ– "updates"
        for output in app.stream(initial_state, config=config, stream_mode="updates"):
            for node_name, node_state in output.items():
                if node_name == "agent":
                    # æ‰“å° AI çš„å›ç­”ï¼ŒåŒæ—¶æ ‡è¯†æ˜¯å“ªä¸ªæ¨¡å‹ç”Ÿæˆçš„
                    last_msg = node_state['messages'][-1]
                    print(f"\n[{current_model_id}]:\n{last_msg.content}")
                elif node_name == "tools":
                    # æ‰“å°å·¥å…·æ‰§è¡Œè¿‡ç¨‹
                    tool_res = node_state['messages'][-1]
                    print(f"\n[ç³»ç»Ÿæ‰§è¡Œç»“æœ]: {tool_res.content}")